
\section{Software Implementation}
The following subsections detail the implementation of the final software solution that has been written to meet the objectives posed previously of this dissertation.

\subsection{Languages and platforms}
The final system has been written entirely using the C\# programming language (version 5.0) with Visual Studio 2015 as the development environment on a Windows 10 system. C\# is an application development language built on the .NET framework. Although any number of programming languages could have been used to implement the solution C\# offered a good compromise for developing a system with both structural rigidity through static typing and object orientation in addition to functionality to allow for rapid prototyping. C\# does this well through use of LINQ, a part of the standard library that provides a large number of higher order functions which allow for operations to be performed over any data structure that implements the built in IEnumerable interface. Given that much of the code within the project performs the same operation on collections of nodes and elements stored in lists, arrays and dictionaries which all implement IEnumerable the ability to write much of the project using this capability dramatically reduced the number of errors encountered and increased development speed.


\subsection{Implementation Methodology}
The growing size of the software meant it was important to work systematically to continuously drive the project in the right direction and avoid the introduction of unnecessary complexity. This was achieved through regularly reviewing and refactoring the code which dramatically helped to reduced the amount of bugs introduced. \\

\noindent
For the duration of the project the spiral methodology was adhered to. This enforced multiple deliverable stages that were concluded with a supervisor meeting every one or two weeks. Adopting the spiral methodology also provided flexibility regarding the order in which tasks were able to take place outside of a spiral iteration. This was necessary when conducting a research driven project where direction of work for subsequent development iterations was largely driven by the  findings of the work in the previous ones. \\

\noindent
Tasks were chosen every week for the project, the number of tasks and their complexity was determined using a combination of factors including their complexity, the criticality of the task e.g. Did it need to be completed for other important tasks to be started and the time available to me as the individual undertaking the project (More tasks typically performed on weeks when less work was due for other modules. \\ 


\subsection{Hierarchical Refinement}
\noindent
Elements within traditional FEA can typically be classified as either triangle or square based elements, each of these provide different strengths and weaknesses when used                                                                                                                                                                                                                                                                                                                                                                                                                                                                         to mesh and solve models. Within industry triangular elements are typically preferable since it is always possible to generate an initial triangular mesh from any arbitrary CAD geometry algorithmically. This is done by simply making smaller triangles until all gaps along the edge of the geometry are filled \cite{DelaunyTriangles}. The same cannot always be said  when meshing using square elements. For proof of the solutions concept however it was concluded that square based elements were preferable to triangular ones since the steps required for a basic refinement are much simpler, just take the corners of an element that already exists, add their corresponding x, y and z components before dividing each component by two to achieve the new midpoint. \\ 

\noindent
In addition to refinement it is also significantly easier to define edges which the ILP rules can be applied to when edges naturally form within a structure through a chain of nodes along the edges of square elements.

%The process of re meshing square based elements 

\noindent
Unfortunately Triangular meshes also generally incur a higher computational cost than an equivalent square element mesh due to added complexity of performing the calculations required to remesh in addition to requiring more elements over a given area to achieve the same accuracy. \\

\noindent
From an implementation standpoint writing a square based remeshing algorithm was also substantially easier if given a mesh of exclusively square elements since the primary task to be performed is to repeatedly divide each element into four sub elements, by contrast methods uses to re mesh triangular meshes are typically more complex and have corresponding initial meshes that are harder to generate manually by human operators \cite{HandMeshing}. \\ 



\subsection{Stress Based Refinement}
To focus meshing in areas of high stress each iteration needed to parse the results file from the previous iterations execution of LISA. LISA result files are in csv format by default and contain the displacements and stresses associated with each node within the model once it has been solved. \\

\noindent
Once the data in the output file has been parsed the nodal values for which displacement is known for can be cross referenced against those in the current model by intersecting the lists of node data on node Ids. An evaluation function is then able to determine whether or not any element handed to it meets the criteria for refinement by simply looking at the sum of the stress at its given nodes. If an element is determined to be over the threshold to justify refinement the elements ``createChildElements()'' method is called to subdivide it further. \\


\subsection{Heuristic/Rule Based Refinement}
Each rule is represented as a function within the implementation, this closely resembles the format presented by Dolsak \cite{DolsakPaper91, DolsakPaper94, appOfILPToFEMeshDesign} \cite{ConsultRuleIntelltSystemFE}. The rules resides within the ``RuleManager'' class and each take a number of the defined edges as parameters. When an instance of the RuleManager is created it parses the edges file provided by the user into a list of edges that the rules can then be executed on. Every rule then checks the properties of a particular edge against properties which have been identified through the ILP learning algorithm as being important when the model executes. In cases where the rules accept more than one edge as an argument the system attempts to apply the rule to each pair of different edges in the edge list giving a time complexity of $O(n^2)$ where n is the total number of defined edges.\\

\noindent
If a rule detects a relationship in the model the edge is assigned a criticality rating as defined by the rule, the value is then used by the meshing procedure to determine how many times it should re mesh the elements along that edge. \\ 
 
The properties that can exist between two edges when compared are the following:
\begin{itemize}
\item Edges opposite one another - the edges run alongside one another closely, look at the distance between each of the corresponding nodes and check whether this distance is less than some threshold amount
\item Edges posses the same form - \colorbox{yellow}{Still need to write about this}
%finish this

\item Edges are considered the same - to meet this requirement both edges must be almost the same length, opposite one another and posses the same form.

\end{itemize}

\noindent
Since this system relied on a persistent definition of edges across multiple refinement iterations another challenge was to correctly redefine edges in terms of the newly created nodes so that after meshing had occurred the rules could be re applied to a refined edge to potentially refine it further.

\begin{figure}[!h]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.9\linewidth]{../Graphics/Rule7Implementation.png}
  \caption{Code implementation of rule 7 provided by Dolsak within the RuleManger class}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.7\linewidth]{../Graphics/Rule7Dolsak.png}
  \caption{Rule 7 as stated by dolsak in his papers \cite{appOfILPToFEMeshDesign}}
  \label{fig:sub2}
\end{subfigure}
\label{fig:test}
\end{figure}


\subsection{Mesh Quality Assessment}
Dittmers rules for computing the quality of both individual elements and the entire mesh are built into their own ``MeshQualtyAssessments" and ``ElementQualityMetrics" classes, the latter of which is encapsulated within an element object, like with refinement this allows each element to assess its own quality removing the need for additional utility classes and static methods. \\

\noindent
%not sure about the last sentence here
Since each element is initialised with the nodes that comprise it, it is also possible to derive all the geometric characteristics and thus its quality metrics upon its initialisation. This allows the metrics for each element to also be calculated upon its initialisation and thus removing the risk of null values being returned when other parts of the system request this information.


%Upon evaluation of the project and concluding that effectiveness of the heuristic relied upon overlap of the %heuristically mesh area with areas of high stress


\subsection{Implementation Challenges}
Implementation of the system was not without its share of challenges, some of which required fundamentally re addressing the approach used to tackle the problem. This section outlines the main instances of such cases during the projects development where as a consequence a notable change to the implementation was made often requiring additional research.

\subsubsection{Fast Node Lookup and Update}
A key requirement for the design of the data model generated by the hierarchical re meshing process was the need to perform fast lookup of nodes already present in the mesh. Lookup is important within the meshing methods as a means of checking whether a node that is about to be created already exists within the model, in the event that no such node already exists a new one can be created however if it does then instead of creating a new node the node that already exists needs to be connected to a node in an adjacent element that is currently being refined. If nodes are not linked correctly form correct elements the physics solver is unable to assume the stress moves through one element to another despite both having nodes at the same coordinates, this results in inaccurate output or potentially an error being thrown by LISA. \\ 

\noindent
This issue arose partly as a result of the systems design, as previously mentioned subdivision for every individual element is the responsibility of that element which from a software engineering perspective is very good since it means the low level meshing process for each different type of element could be written within that elements class. This avoids the need for much heavier generalised refinement classes that would have needed to know how to perform the meshing for all elements in the model at once and for each of the different potential element types. A consequence of this was despite every Element being capable of meshing itself perfectly adjacent elements that also requiring refinement needed the ability to reconnect the new nodes along their edges to those that have been created by the adjacent element, this can be seen below in figure 9. \\ 


\begin{figure}[!h]
  \centerline{\includegraphics[width=100mm , scale=1]{../Graphics/nodeLinking.png}}
  \caption{The need for an element to check for existing adjacent nodes when subdividing itself during refinement,\\ \\
  	Orange Nodes - An original node for one or more elements \\
	Red Nodes - new nodes made by Elem A \\
	Purple Nodes - new nodes made by Elem B \\
  }
  \label{fig:h-refinementImp}
\end{figure}


\noindent
The solution to this problem was to store all the nodes in the mesh model within a C\# dictionary structure a reference to which is passed to each element within the model. The dictionary can be indexed using a Tuple of the x, y and z coordinates for the new potential element which will either return a node already at that location or indicate that no such node exists, in which case that element is then responsible for creating the node as its first instance. Dictionaries in C\# represent a generalised instance of a hash table ensuring that lookup and insert are both constant time on average.

\subsubsection{Sorting Element Nodes}
One issue faced when working with LISA was an interface requirement specified requiring nodes for each type of element to be sorted in a specific geometric order. The general rule for node ordering within LISA is to have them form a perimeter around the edge of an element in 3d space without edges crossing one another internal to the element. \\

\begin{figure}[!h]
  \centerline{\includegraphics[width=50mm , scale=1]{../Graphics/BadlyOrderedNodes.png}}
  \caption{Element with 3d skew resulting in edges between diagonals being shortest by a small amount.
  }
  \label{fig:h-refinementImp}
\end{figure}

\noindent
When addressing this problem for simple models constructed from Quad4 elements the most straightforward approach was to simply traverse each of the nodes in the order specified by LISA and with the resulting traversal list being ordered for LISA. The resulting traversal process resembles the following: \\


\begin{algorithm}[H]
 \While{$\exists node \in UnstortedNodes$}{
  Get distance between current node and the next two nodes in UnstortedNodes\;
  
  \eIf{Nodes left ==  1}{
  
  }

  \eIf{distanceToNode1 < distanceToNode2}{
  currentNode Node1\;
  	sortedNodes.Add(Node1)\;
  	UnstortedNodes.Remove(Node1)\;
   }
   {
    currentNode Node2\;
  	sortedNodes.Add(Node2)\;
  	UnstortedNodes.Remove(Node2)\;
  }
 }
 \caption{How to write algorithms}
\end{algorithm}

\noindent
For the most part this approach was both fast and correct for Quad4 elements although in cases where elements were particularly skewed in 3d space it was sometimes possible for the internal diagonals to be shorter than the actual sides as seen in Figure 9 below. This proved to be a significant flaw in the method and brought about the realisation that a reliable approach would not be able to depend simply upon highly variable properties such as node distances. \\ 

\begin{figure}[!h]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.9\linewidth]{../Graphics/SkewedElementIssues.png}
  \caption{Dividing a Quad4 along planes to establish each node as a corner point}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.7\linewidth]{../Graphics/ElementSkewOnBridge.png}
  \caption{Skew in bridge model elements resulting in rejection of the model by LISA}
  \label{fig:sub2}
\end{subfigure}
\caption{Incorrectly sorted elements arising from failure of traversal routine for skewed elements}
\label{fig:test}
\end{figure}



\noindent
Attempting to arrive at a more general solution focus was directed towards sorting the more complex Hex8 element type as this represented a more complete instance of the problem. Analysis of this led to the realisation that the most important task in sorting nodes for an arbitrary type is to simply establish the corner nodes relative to that type. Having established corners correctly sorting then simply required adding them to a list in the order specified by LISA. \\

\noindent
The subsequent method which was used to successfully establish corners for both Quad4 and Hex8 elements was to split nodes for each element using planes running along the x, y and z axis as can be seen in Figure 9 below.

\begin{figure}[!h]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.9\linewidth]{../Graphics/SortingQuad4.png}
  \caption{Dividing a Quad4 along planes to establish each node as a corner point}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.7\linewidth]{../Graphics/SortingHex8.png}
  \caption{Dividing a Hex8 along planes to establish each node as a corner point}
  \label{fig:sub2}
\end{subfigure}
\caption{Splitting Element points using x, y and z planes in order to perform ordering for LISA}
\label{fig:test}
\end{figure}

\noindent
Although this approach resolved the initial problems resulting from simply trying to traverse the nodes it did not offer a strong general case solution to the problem with the code for a Hex8 element needing to be significantly different and more complex than that of a Quad4 and with the potential for the most complex FE element types such as wedge15, hex20, and pyr13 requiring implementations with even greater number of plane divisions and groupings in order to successfully identify every node. \\ 


\noindent
Having already devised two solutions it seemed likely that there would be some body of research surrounding the problem worth investigating. with research leading to a set of possible alternaives known as convex hull algorithms. As the name suggests the goal of a convex hull algorithm is to generate a convex hull, convex hulls have several definitions but the simplest of these as described by \cite{ConvexHulls} is for a set of points in some space a subset S of those points is convex if for any two points P and Q  inside S the line between the two should also be inside S. This is directly applicable in the case of quad4 elements where the LISA sort order is the convex hull of the points, in the case of more complex elements the algorithm can be applied repeatedly to different faced divided though plane splitting before sorting the nodes at the end with knowledge of node ordering within each individual face. \\ 

\begin{figure}[!h]
  \centerline{\includegraphics[width=100mm , scale=1]{../Graphics/ConvexHullGraphic.png}}
  \caption{Illustration of convex hull definition, imace source: \cite{ConvexHulls}
  }
  \label{fig:h-refinementImp}
\end{figure}


\noindent
Having considered several convex hull algorithms including Graham scan \cite{GrahamScan} $O(n\ log\ n)$ and brute force scan $O(n^4)$ \cite{ConvexHulls} \cite{BruteConvex} before deciding to trial the following C\# implementation of the Monotone Chain algorithm, also $O(n\ log\ n)$ \cite{CSharpConvexHull}. \\ 

\noindent
The Monotone Chain algorithm algorithm was developed shortly after Graham scan and builds upon the concepts introduced in the former. Graham's scan works by initially finding the point in the data set with the lowest y coordinate which can be called P. Having found this point the other points in the set are sorted based on the angle created between them and P. Combining both these steps gives a complexity of $O(n\ log\ n)$, with  $O(n)$ to find P and $O(n\ log\ n)$ to perform a general sort of the angles. Moving through each point in the sorted array Graham scan determines whether moving to this point results in making a right or left hand turn based on the two previous points. If a right turn is made then the second do last point has caused a concave shape which has violated the requirement of the convex hull path. In this scenario is repeated the algorithm excludes the point from the convex set and resumes with the previous two points being those on the path before the rejected point. A stack structure is therefore typically used to keep track of the point ordering as is the case with the Monotone Chain implementation within the system \cite{ChainHull}. \\ 

\noindent
The Monotone Chain algorithm performs essentially the same procedure however instead of sorting using simply y values Monoton Chain sorts using both x and y values. This allows the algorithm to sort the points in two separate groups which form the top and the bottom of the hull and a reduction in the complexity of the sort comparison function. \\ \\ 

%Monotone chain
\begin{algorithm}[H]

%\begin{algorithmic}[1]
%\Procedure{Monotone Chain algorithm}{}


	Sort the points of P by x-coordinate (in case of a tie, sort by y-coordinate)\;
	%\State $\textit{U} \gets \text{Empty List}\textit{string}$
	%\State $\textit{L} \gets \text{Empty List}\textit{string}$
	Make two empty lists I and L
	Lists hold vertices of upper and lower hull\;
	
	\While{i = 1; i < n; i++}{
		\While{L contains at least two points and the sequence of last two points of L and the point P[i] does not make a counter clockwise turn}{
		Remove the last Point from L\;
		}
	}
	
	\While{i = n; i > 1; i- -}{
		\While{L contains at least two points and the sequence of last two points of L and the point P[i] does not make a counter clockwise turn}{
		
		Remove the last Point from U\;
		
		}
	}
	
Remove the last point of each list (it's the same as the first point of the other list).\;
Concatenate L and U to obtain the convex hull of P.\;
Points in the result will be listed in counter-clockwise order.\;

\caption{Monotone Chain algorithm for generating convex hull, pseudocode description credit: \cite{MonotoneChain}}\label{MonotoneChain}
\end{algorithm}

%\cite{MonotoneChain}

%algo 2


\noindent
\newline
%This method has $O(n\ log\ n)$ time complexity however due to the size of n being 4 in all cases the complexity of sorting an individual element is constant, with the overall complexity of sorting all elements in the model being $O(n)$ where n is the number of elements. \\ 

%A key drawback of both Graham scan and Monotone Chain is their limitation to 2D space. Despite the existance of algorithms for generating convex hulls in n dimensions such as \cite{} and

\noindent
The additional complexity of implementing a 3D convex hull algorithm meant it was much easier to experiment the with the approach as a potential solution to the problem using a 2D implementation by simply reducing the problem to a 2D equivalent. This was for quad4 elements by calculating the maximum delta between the max and min value on each axis and eliminating the axis with the smallest delta. These new 2D points could be given to the algorithm which when used in conjunction with the approach already taken was able to solve all instances of node ordering within the models. The only instances in which this approach failed were where highly elements would lie on a perfectly diagonal plane resulting in two axis of elimination using this method. This problem was avoid however by using the basic traversal to sort these elements. \\ 



%Think about putting this under evaluation or removing it instead
\subsubsection{Attempts to Automatically Define Edges Within Models}
As discussed under evaluation another central faced was identifying where the system behaved poorly as a result of weakness in the underlying methods or poor design and implementation  as opposed to poor edge definitions as input for the heuristic method. One way to avoid this issue of evaluation would be to remove all user intervention from the process besides the configuration of the initial model.

\noindent
The obvious way by which to do this was automatic identification of interesting edges which would normally be the responsibility of the trained engineer. Such an approach initially seems promising since the quality of edges as with the mesh can be judged using a small number of key properties. For example it is known that edge importance directly correlates with the size of the edge and how much force is applied near it, since this information exists within the data model it should be possible to identify edges from it and generate them automatically for Dolsaks rules to process.  \\

\noindent
In practice there were multiple complications surrounding this, several of these arise from ambiguity in Dolsaks paper regarding what constitutes a an edge which is for example "long" or "important". As a result edges are only able to be defined to the extent that a user has confidence in their understanding of these concepts. \\ 

Assuming it is possible to generate a system capable of producing edges of interest for a model the problem of evaluating system correctness persists. Bad results then indicate that one of the two systems is broken but provide no clear indication as to which one is.


Having considered a method using these properties of the mesh several days were then spent attempting to implement it with poor success achieved.
